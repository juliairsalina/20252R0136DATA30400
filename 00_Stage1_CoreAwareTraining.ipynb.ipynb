{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cb5aa4-74b0-4467-aa73-9ba425e9cbce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T02:51:43.851022Z",
     "iopub.status.busy": "2025-12-20T02:51:43.850740Z",
     "iopub.status.idle": "2025-12-20T02:51:43.989768Z",
     "shell.execute_reply": "2025-12-20T02:51:43.989124Z",
     "shell.execute_reply.started": "2025-12-20T02:51:43.851003Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 1. SETUP: Load Preprocessed Features\n",
    "# ==========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the tensors we saved in the Preprocessing notebook\n",
    "data = torch.load(\"preprocessed_features.pth\", weights_only=False)\n",
    "X_train = data[\"X_train\"] # [N, 768]\n",
    "y_all = data[\"y_all\"]     # [N, 531] - Full hierarchy\n",
    "y_core = data[\"y_core\"]   # [N, 531] - Just keywords\n",
    "ancestor_list = data[\"ancestors\"]\n",
    "\n",
    "E_label_768 = data[\"E_label_768\"] \n",
    "if isinstance(E_label_768, np.ndarray):\n",
    "    E_label_768 = torch.from_numpy(E_label_768)\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, y, y_c):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.y_c = y_c\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"X\": self.X[idx], \"y\": self.y[idx], \"y_core\": self.y_c[idx]}\n",
    "\n",
    "train_loader = DataLoader(MultiLabelDataset(X_train, y_all, y_core), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e7d40c-b8c7-4b57-8b10-71e92523ab35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T02:51:46.972432Z",
     "iopub.status.busy": "2025-12-20T02:51:46.972154Z",
     "iopub.status.idle": "2025-12-20T02:51:46.980921Z",
     "shell.execute_reply": "2025-12-20T02:51:46.980366Z",
     "shell.execute_reply.started": "2025-12-20T02:51:46.972411Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 2. ARCHITECTURE: Multi-Head GATv2 + Path\n",
    "# ==========================================================\n",
    "class MultiHeadPathLabelAttn(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.w_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.w_k = nn.Linear(dim, dim, bias=False)\n",
    "        self.v_attn = nn.Linear(self.head_dim, 1, bias=False)\n",
    "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.gate = nn.Parameter(torch.ones(1) * 0.5)\n",
    "\n",
    "    def forward(self, E, ancestors):\n",
    "        L, d = E.shape\n",
    "        Q = self.w_q(E).view(L, self.num_heads, self.head_dim)\n",
    "        K = self.w_k(E).view(L, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(E).view(L, self.num_heads, self.head_dim)\n",
    "        \n",
    "        out = torch.zeros_like(E)\n",
    "        for c in range(L):\n",
    "            anc = ancestors[c]\n",
    "            if not anc:\n",
    "                out[c] = E[c]; continue\n",
    "            \n",
    "            # GATv2 Dynamic scoring\n",
    "            q_c = Q[c:c+1] \n",
    "            k_a = K[anc]\n",
    "            scores = self.v_attn(F.leaky_relu(q_c + k_a, 0.2)).squeeze(-1)\n",
    "            attn = F.softmax(scores, dim=0) \n",
    "            \n",
    "            msg = (attn.unsqueeze(-1) * V[anc]).sum(dim=0).view(d)\n",
    "            # Gated residual to balance original text vs hierarchy info\n",
    "            out[c] = (self.gate * E[c]) + ((1.0 - self.gate) * msg)\n",
    "        return self.ln(out)\n",
    "\n",
    "class ProposedClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels, emb_dim, E_label_768, ancestors):\n",
    "        super().__init__()\n",
    "        self.doc_proj = nn.Sequential(nn.Linear(input_dim, emb_dim), nn.ReLU(), nn.Dropout(0.3))\n",
    "        self.label_proj = nn.Linear(768, emb_dim, bias=False)\n",
    "        self.register_buffer(\"E_text\", E_label_768.float())\n",
    "        self.label_attn = MultiHeadPathLabelAttn(emb_dim, num_heads=4)\n",
    "        self.ancestors = ancestors\n",
    "\n",
    "    def forward(self, X):\n",
    "        h = self.doc_proj(X)\n",
    "        E = self.label_attn(self.label_proj(self.E_text), self.ancestors)\n",
    "        return h @ E.t()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd3fdf3-d5cb-45cd-9f18-008ec965ec36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T02:50:53.513144Z",
     "iopub.status.busy": "2025-12-20T02:50:53.512708Z",
     "iopub.status.idle": "2025-12-20T02:50:53.532617Z",
     "shell.execute_reply": "2025-12-20T02:50:53.531896Z",
     "shell.execute_reply.started": "2025-12-20T02:50:53.513109Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'E_label_768' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 3. TRAINING LOOP: Core-Aware & Gradient Clipping\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ==========================================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# TWEAK: Set weight for auxiliary (expanded) labels\u001b[39;00m\n\u001b[1;32m      5\u001b[0m W_AUX \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.45\u001b[39m \n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m ProposedClassifier(\u001b[38;5;241m768\u001b[39m, \u001b[38;5;241m531\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[43mE_label_768\u001b[49m, ancestor_list)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      9\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'E_label_768' is not defined"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 3. TRAINING LOOP: Core-Aware & Gradient Clipping\n",
    "# ==========================================================\n",
    "# TWEAK: Set weight for auxiliary (expanded) labels\n",
    "W_AUX = 0.45 \n",
    "\n",
    "model = ProposedClassifier(768, 531, 256, E_label_768, ancestor_list).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        Xb, y_a, y_c = batch[\"X\"].to(device), batch[\"y\"].to(device), batch[\"y_core\"].to(device)\n",
    "        \n",
    "        logits = model(Xb)\n",
    "        loss_elem = F.binary_cross_entropy_with_logits(logits, y_a, reduction='none')\n",
    "        \n",
    "        # --- CORE-AWARE WEIGHTING ---\n",
    "        y_aux = (y_a - y_c).clamp(0, 1)\n",
    "        weight = torch.ones_like(loss_elem) + (W_AUX - 1.0) * y_aux \n",
    "        \n",
    "        loss = (loss_elem * weight).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # PREVENT EXPLODING GRADIENTS\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"stage1_teacher_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
