{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00bbda-d3ab-4af9-8dcf-49cef3bfbb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 1. SETUP: Load Features and Teacher Model\n",
    "# ==========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Preprocessed Data\n",
    "data = torch.load(\"preprocessed_features.pth\")\n",
    "X_train = data[\"X_train\"].to(device)\n",
    "Y_silver = data[\"y_all\"].to(device) # Original silver labels\n",
    "E_label_768 = data[\"E_label_768\"].to(device)\n",
    "ancestors = data[\"ancestor_list\"]\n",
    "\n",
    "# Load Teacher (Stage 1 Model)\n",
    "teacher = ProposedClassifier(768, 531, 256, E_label_768, ancestors).to(device)\n",
    "teacher.load_state_dict(torch.load(\"stage1_teacher_model.pth\"))\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4439d1-9284-450b-9ab4-4c697ccaa5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 2. GENERATE SOFT TARGETS (Distillation)\n",
    "# ==========================================================\n",
    "# TWEAKS for Stage 2\n",
    "LAMBDA = 0.7  # How much to trust the teacher vs silver labels\n",
    "TEMP = 2.5    # Temperature scaling to smooth distributions\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_q_soft(model, X, Y_hard, temp=2.5, lam=0.7):\n",
    "    # Predict probabilities with temperature scaling\n",
    "    logits = model(X)\n",
    "    P_teacher = torch.sigmoid(logits / temp)\n",
    "    \n",
    "    # Mix teacher intelligence with original silver labels\n",
    "    Q_soft = (1.0 - lam) * Y_hard + lam * P_teacher\n",
    "    return Q_soft\n",
    "\n",
    "Q_soft = generate_q_soft(teacher, X_train, Y_silver, temp=TEMP, lam=LAMBDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f6859-3666-45eb-bc6b-f8dfa9dd48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 3. CONFIDENCE-AWARE WEIGHTING LOGIC\n",
    "# ==========================================================\n",
    "def get_confidence_weight(q, gamma=2.5):\n",
    "    \"\"\"\n",
    "    Assigns higher weight to certain predictions (near 0 or 1)\n",
    "    and zero weight to uncertain ones (near 0.5).\n",
    "    \"\"\"\n",
    "    return (torch.abs(q - 0.5) * 2.0) ** gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b901aa-120f-4014-80d0-2dce00681af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 4. STAGE 2 TRAINING LOOP\n",
    "# ==========================================================\n",
    "# Initialize a fresh STUDENT model\n",
    "student = ProposedClassifier(768, 531, 256, E_label_768, ancestors).to(device)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=3e-4) # Lower LR for refinement\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    student.train()\n",
    "    # Iterate through batches of X_train and Q_soft\n",
    "    # ... (batching logic) ...\n",
    "    \n",
    "    probs_student = torch.sigmoid(student(batch_X))\n",
    "    \n",
    "    # Apply Confidence-Aware Loss\n",
    "    w = get_confidence_weight(batch_Q)\n",
    "    loss_elem = F.binary_cross_entropy(probs_student, batch_Q, reduction='none')\n",
    "    loss = (loss_elem * w).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae96057-c79d-4892-b620-d4441169497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# FINAL KAGGLE SUBMISSION: Stage 2 Student Inference\n",
    "# ==========================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Prepare Model for Evaluation\n",
    "student.eval()\n",
    "test_probs = []\n",
    "\n",
    "# 2. Batch Inference on Test Set\n",
    "# We use batching to avoid OOM (Out of Memory) issues on the GPU\n",
    "BATCH_SIZE = 256\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    \n",
    "    for i in tqdm(range(0, len(X_test_tensor), BATCH_SIZE), desc=\"Final Inference\"):\n",
    "        batch_x = X_test_tensor[i : i + BATCH_SIZE]\n",
    "        # Pass through our Multi-Head GAT+Path Student model\n",
    "        logits = student(batch_x)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        test_probs.append(probs.cpu().numpy())\n",
    "\n",
    "# Combine all batch probabilities into one matrix [NumTestSamples, 531]\n",
    "all_test_probs = np.vstack(test_probs)\n",
    "\n",
    "# 3. Adaptive Label Selection (2-3 Label Constraint)\n",
    "def get_final_labels(prob_vector, thr=0.35, min_l=2, max_l=3):\n",
    "    \"\"\"\n",
    "    Selects labels based on a threshold while forcing a count between min_l and max_l.\n",
    "    \"\"\"\n",
    "    # Sort indices by probability descending\n",
    "    idx = np.argsort(prob_vector)[::-1]\n",
    "    \n",
    "    # Select labels that pass the threshold\n",
    "    chosen = [i for i in idx[:50] if prob_vector[i] >= thr]\n",
    "    \n",
    "    # FORCE CONSTRAINT: If fewer than 2, take the top 2\n",
    "    if len(chosen) < min_l:\n",
    "        chosen = idx[:min_l].tolist()\n",
    "        \n",
    "    # FORCE CONSTRAINT: If more than 3, take the top 3\n",
    "    # This maximizes F1 by not over-predicting broad categories\n",
    "    final_selection = sorted(chosen[:max_l])\n",
    "    \n",
    "    return \",\".join(map(str, final_selection))\n",
    "\n",
    "# 4. Generate Final Prediction Strings\n",
    "print(\"Applying adaptive thresholding...\")\n",
    "final_pred_strs = [get_final_labels(p, thr=0.35) for p in all_test_probs]\n",
    "\n",
    "# 5. Create Submission DataFrame\n",
    "# Ensure 'test_ids' is the list of IDs from your test corpus load step\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_ids,\n",
    "    \"label\": final_pred_strs\n",
    "})\n",
    "\n",
    "# 6. Save to CSV\n",
    "OUTPUT_FILE = \"2023320344_Final.csv\"\n",
    "submission.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"SUCCESS: Submission saved to {OUTPUT_FILE}\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
