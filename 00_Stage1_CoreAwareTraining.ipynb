{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31cb5aa4-74b0-4467-aa73-9ba425e9cbce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T07:43:22.440066Z",
     "iopub.status.busy": "2025-12-20T07:43:22.439798Z",
     "iopub.status.idle": "2025-12-20T07:43:22.581467Z",
     "shell.execute_reply": "2025-12-20T07:43:22.580888Z",
     "shell.execute_reply.started": "2025-12-20T07:43:22.440045Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 1. SETUP: Imports, Devices, and Data Loading\n",
    "# ==========================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import copy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_CLASSES = 531 # Ensure this is defined globally\n",
    "\n",
    "# Load the artifacts from the Preprocessing notebook\n",
    "data = torch.load(\"preprocessed_features2.pth\", weights_only=False)\n",
    "\n",
    "X_train = data[\"X_train\"] \n",
    "y_all = data[\"y_all\"]     \n",
    "y_core = data[\"y_core\"]   \n",
    "ancestor_list = data[\"ancestors\"]\n",
    "E_label_768 = data[\"E_label_768\"] \n",
    "\n",
    "# Convert label embeddings to tensor if they were saved as numpy\n",
    "if isinstance(E_label_768, np.ndarray):\n",
    "    E_label_768 = torch.from_numpy(E_label_768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ab6e849-4379-4cf9-ac9b-f7c6f0195c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T07:43:24.320709Z",
     "iopub.status.busy": "2025-12-20T07:43:24.320441Z",
     "iopub.status.idle": "2025-12-20T07:43:24.355567Z",
     "shell.execute_reply": "2025-12-20T07:43:24.354992Z",
     "shell.execute_reply.started": "2025-12-20T07:43:24.320691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data ready: 25063 train, 4424 val samples.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 2. DATA LOADERS & SPLITTING\n",
    "# ==========================================================\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, y, y_c):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.y_c = y_c\n",
    "    def __len__(self): \n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"X\": self.X[idx], \n",
    "            \"y\": self.y[idx], \n",
    "            \"y_core\": self.y_c[idx]\n",
    "        }\n",
    "\n",
    "# Split indices for validation (15%)\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(X_train)), \n",
    "    test_size=0.15, \n",
    "    random_state=42, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_ds = MultiLabelDataset(X_train[train_idx], y_all[train_idx], y_core[train_idx])\n",
    "val_ds   = MultiLabelDataset(X_train[val_idx],   y_all[val_idx],   y_core[val_idx])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\" Data ready: {len(train_ds)} train, {len(val_ds)} val samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64e7d40c-b8c7-4b57-8b10-71e92523ab35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T07:43:26.825899Z",
     "iopub.status.busy": "2025-12-20T07:43:26.825615Z",
     "iopub.status.idle": "2025-12-20T07:43:26.836512Z",
     "shell.execute_reply": "2025-12-20T07:43:26.835887Z",
     "shell.execute_reply.started": "2025-12-20T07:43:26.825878Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 3. ARCHITECTURE: Multi-Head GATv2 + Path Attention\n",
    "# ==========================================================\n",
    "class MultiHeadPathLabelAttn(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, dropout=0.2): # Increased to 8 heads for Strategy B\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(dim, dim, bias=False)\n",
    "        self.w_k = nn.Linear(dim, dim, bias=False)\n",
    "        self.v_attn = nn.Linear(self.head_dim, 1, bias=False)\n",
    "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
    "        \n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Learnable gate to balance self-information vs. hierarchy information\n",
    "        self.gate = nn.Parameter(torch.ones(1) * 0.5)\n",
    "\n",
    "    def forward(self, E, ancestors):\n",
    "        L, d = E.shape\n",
    "        Q = self.w_q(E).view(L, self.num_heads, self.head_dim)\n",
    "        K = self.w_k(E).view(L, self.num_heads, self.head_dim)\n",
    "        V = self.v_proj(E).view(L, self.num_heads, self.head_dim)\n",
    "        \n",
    "        out = torch.zeros_like(E)\n",
    "        for c in range(L):\n",
    "            anc = ancestors[c]\n",
    "            if not anc:\n",
    "                out[c] = E[c]\n",
    "                continue\n",
    "            \n",
    "            # GATv2 Dynamic Attention\n",
    "            q_c = Q[c:c+1] \n",
    "            k_a = K[anc]\n",
    "            \n",
    "            # Compute scores: LeakyReLU(Q + K) is more expressive than standard GAT\n",
    "            scores = self.v_attn(F.leaky_relu(q_c + k_a, 0.2)).squeeze(-1)\n",
    "            attn = F.softmax(scores, dim=0) \n",
    "            \n",
    "            # Aggregate hierarchy message\n",
    "            msg = (attn.unsqueeze(-1) * V[anc]).sum(dim=0).view(d)\n",
    "            \n",
    "            # Apply Gating\n",
    "            out[c] = (self.gate * E[c]) + ((1.0 - self.gate) * msg)\n",
    "            \n",
    "        # Strategy B: Added Residual Connection (out + E) before LayerNorm\n",
    "        # This ensures specific label meanings aren't \"washed out\" by broad parent info\n",
    "        return self.ln(self.dropout(out) + E)\n",
    "\n",
    "class ProposedClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels, emb_dim, E_label_768, ancestors):\n",
    "        super().__init__()\n",
    "        self.doc_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, emb_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.label_proj = nn.Linear(768, emb_dim, bias=False)\n",
    "        self.register_buffer(\"E_text\", E_label_768.float())\n",
    "        \n",
    "        # Consistent num_heads with the Attention class\n",
    "        self.label_attn = MultiHeadPathLabelAttn(emb_dim, num_heads=8)\n",
    "        self.ancestors = ancestors\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 1. Project product BERT features\n",
    "        h = self.doc_proj(X)\n",
    "        \n",
    "        # 2. Refine label embeddings using hierarchy attention\n",
    "        # Label proj maps BERT label space (768) to joint emb_dim (256)\n",
    "        E = self.label_attn(self.label_proj(self.E_text), self.ancestors)\n",
    "        \n",
    "        # 3. Compute compatibility (Dot Product)\n",
    "        return h @ E.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "790a2b15-0c3a-40a2-9eda-1eb1029d7b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T07:43:32.255531Z",
     "iopub.status.busy": "2025-12-20T07:43:32.255257Z",
     "iopub.status.idle": "2025-12-20T07:43:32.261333Z",
     "shell.execute_reply": "2025-12-20T07:43:32.260517Z",
     "shell.execute_reply.started": "2025-12-20T07:43:32.255511Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 4. EVALUATION HELPER\n",
    "# ==========================================================\n",
    "def evaluate_f1(model, loader, thr=0.35):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            Xb, yb = batch[\"X\"].to(device), batch[\"y\"].to(device)\n",
    "            probs = torch.sigmoid(model(Xb)).cpu().numpy()\n",
    "            for p in probs:\n",
    "                idx = np.argsort(p)[::-1]\n",
    "                chosen = [i for i in idx[:50] if p[i] >= thr]\n",
    "                if len(chosen) < 2: chosen = idx[:2].tolist()\n",
    "                \n",
    "                vec = np.zeros(NUM_CLASSES)\n",
    "                vec[chosen[:3]] = 1 # Top 3 max constraint\n",
    "                all_preds.append(vec)\n",
    "            all_targets.append(yb.cpu().numpy())\n",
    "    return f1_score(np.vstack(all_targets), np.vstack(all_preds), average='samples', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2b0ca-d8ab-467d-b3ea-beff69f7555a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-20T07:43:34.034826Z",
     "iopub.status.busy": "2025-12-20T07:43:34.034526Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 392/392 [03:22<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.0165 | Val F1: 0.1163\n",
      "  ⭐ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 392/392 [03:23<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.0099 | Val F1: 0.1592\n",
      "  ⭐ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 392/392 [03:24<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.0095 | Val F1: 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 392/392 [03:23<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.0090 | Val F1: 0.1577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 392/392 [03:25<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.0088 | Val F1: 0.1809\n",
      "  ⭐ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 392/392 [03:23<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss: 0.0086 | Val F1: 0.1763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 392/392 [03:22<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss: 0.0084 | Val F1: 0.1974\n",
      "  ⭐ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 392/392 [03:25<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.0082 | Val F1: 0.1953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 392/392 [03:23<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.0079 | Val F1: 0.2038\n",
      "  ⭐ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 392/392 [03:20<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss: 0.0078 | Val F1: 0.2077\n",
      "  ⭐ Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 392/392 [03:24<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5. STAGE 1 TRAINING: Core-Aware & Gradient Clipping (REFINED)\n",
    "# ==========================================================\n",
    "# Strategy C: Lower W_AUX even further (e.g., 0.1 to 0.3) \n",
    "# This tells the model: \"Core labels are 10x more important than expanded ancestors\"\n",
    "W_AUX = 0.2  \n",
    "best_val_f1 = 0.0\n",
    "\n",
    "# Re-instantiate with the new 8-head Architecture\n",
    "model = ProposedClassifier(768, NUM_CLASSES, 256, E_label_768, ancestor_list).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=12)\n",
    "\n",
    "for epoch in range(1, 13): # Increased epochs slightly for the new architecture\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        Xb, y_a, y_c = batch[\"X\"].to(device), batch[\"y\"].to(device), batch[\"y_core\"].to(device)\n",
    "        \n",
    "        logits = model(Xb)\n",
    "        loss_elem = F.binary_cross_entropy_with_logits(logits, y_a, reduction='none')\n",
    "        \n",
    "        # Strategy C Logic: \n",
    "        # y_aux are labels that are in y_all but NOT in y_core\n",
    "        y_aux = (y_a - y_c).clamp(0, 1)\n",
    "        \n",
    "        # We assign a weight of 1.0 to Core labels (y_c) \n",
    "        # and a weight of W_AUX (0.2) to auxiliary labels (y_aux)\n",
    "        weight = torch.ones_like(loss_elem)\n",
    "        weight = torch.where(y_aux == 1, torch.tensor(W_AUX).to(device), weight)\n",
    "        \n",
    "        loss = (loss_elem * weight).mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    current_f1 = evaluate_f1(model, val_loader)\n",
    "    print(f\"Epoch {epoch} | Loss: {total_loss/len(train_loader):.4f} | Val F1: {current_f1:.4f}\")\n",
    "    \n",
    "    if current_f1 > best_val_f1:\n",
    "        best_val_f1 = current_f1\n",
    "        torch.save(model.state_dict(), \"stage1_teacher_model1.pth\")\n",
    "        print(\"  ⭐ Best model saved!\")\n",
    "\n",
    "print(f\"✅ Stage 1 Complete. Best Val F1: {best_val_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESCI Environment (Python 3.9)",
   "language": "python",
   "name": "esci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
